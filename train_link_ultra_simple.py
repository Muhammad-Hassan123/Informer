#!/usr/bin/env python3
"""
ULTRA-OPTIMIZED LINK Training - Simplified Version for Windows
Works reliably with your data setup
"""

import argparse
import os
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

from exp.exp_informer import Exp_Informer

def advanced_data_preprocessing(df):
    """
    Advanced data preprocessing for maximum accuracy
    """
    print("ðŸ”§ Applying advanced preprocessing...")
    
    # 1. Technical indicators
    df['SMA_5'] = df['Close'].rolling(window=5).mean()
    df['SMA_20'] = df['Close'].rolling(window=20).mean()
    df['EMA_12'] = df['Close'].ewm(span=12).mean()
    df['RSI'] = calculate_rsi(df['Close'], 14)
    df['MACD'] = df['EMA_12'] - df['Close'].ewm(span=26).mean()
    
    # 2. Volatility indicators
    df['Price_Change'] = df['Close'].pct_change()
    df['Volatility'] = df['Price_Change'].rolling(window=20).std()
    df['High_Low_Ratio'] = df['High'] / df['Low']
    df['Volume_SMA'] = df['Volume'].rolling(window=10).mean()
    
    # 3. Price patterns
    df['Upper_Shadow'] = df['High'] - df[['Open', 'Close']].max(axis=1)
    df['Lower_Shadow'] = df[['Open', 'Close']].min(axis=1) - df['Low']
    df['Body_Size'] = abs(df['Close'] - df['Open'])
    
    # 4. Market structure
    df['Support'] = df['Low'].rolling(window=20).min()
    df['Resistance'] = df['High'].rolling(window=20).max()
    df['Price_Position'] = (df['Close'] - df['Support']) / (df['Resistance'] - df['Support'])
    
    # 5. Fill NaN values with forward fill
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    print(f"âœ… Added {len(df.columns) - 11} technical indicators")
    return df

def calculate_rsi(prices, window=14):
    """Calculate Relative Strength Index"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

def prepare_ultra_optimized_data(csv_file, output_file):
    """
    Ultra-optimized data preparation with advanced features
    """
    print(f"ðŸš€ Ultra-optimized data preparation from: {csv_file}")
    print("=" * 70)
    
    try:
        df = pd.read_csv(csv_file)
        print(f"âœ… File loaded: {len(df)} rows")
        
        if len(df) < 800:
            print(f"âŒ Need at least 800 rows, got {len(df)}")
            return False
            
        # Take first 800 rows
        df_800 = df.head(800).copy()
        
        # Convert timestamps (handle both date strings and milliseconds)
        try:
            # Try parsing as date string first (like "2022-10-27")
            df_800['date'] = pd.to_datetime(df_800['Open Time'])
            print(f"âœ… Detected date string format: {df_800['Open Time'].iloc[0]}")
        except Exception as e:
            try:
                # Check if it's numeric (milliseconds)
                test_val = df_800['Open Time'].iloc[0]
                if str(test_val).isdigit() and len(str(test_val)) >= 10:
                    df_800['date'] = pd.to_datetime(df_800['Open Time'], unit='ms')
                    print(f"âœ… Detected milliseconds format: {df_800['Open Time'].iloc[0]}")
                else:
                    # Try different date formats
                    date_formats = ['%Y-%m-%d', '%Y/%m/%d', '%d-%m-%Y', '%d/%m/%Y', '%m-%d-%Y', '%m/%d/%Y']
                    success = False
                    for fmt in date_formats:
                        try:
                            df_800['date'] = pd.to_datetime(df_800['Open Time'], format=fmt)
                            print(f"âœ… Detected date format {fmt}: {df_800['Open Time'].iloc[0]}")
                            success = True
                            break
                        except:
                            continue
                    if not success:
                        raise Exception(f"Could not parse date format: {test_val}")
            except Exception as e2:
                print(f"âŒ Could not parse timestamp format: {df_800['Open Time'].iloc[0]}")
                print(f"âŒ Error details: {str(e2)}")
                return False
        
        # Basic OHLCV columns
        base_columns = ['date', 'Open', 'High', 'Low', 'Close', 'Volume']
        
        # Add volume indicators if available
        volume_cols = ['Quote Asset Volume', 'Number of Trades', 'Taker Buy Base', 'Taker Buy Quote']
        for col in volume_cols:
            if col in df_800.columns:
                base_columns.append(col)
        
        df_processed = df_800[base_columns].copy()
        
        # Apply advanced preprocessing
        df_processed = advanced_data_preprocessing(df_processed)
        
        # Sort by date
        df_processed = df_processed.sort_values('date').reset_index(drop=True)
        
        # Data quality checks
        print(f"ðŸ“Š Final shape: {df_processed.shape}")
        print(f"ðŸ“… Date range: {df_processed['date'].min()} to {df_processed['date'].max()}")
        
        # Save processed data
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
        df_processed.to_csv(output_file, index=False)
        
        print(f"ðŸŽ‰ Ultra-optimized data saved to: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def calculate_advanced_confidence(predictions_ensemble, historical_errors=None):
    """
    Advanced confidence calculation using ensemble variance
    """
    # Ensemble variance (disagreement between models)
    ensemble_variance = np.var(predictions_ensemble, axis=0)
    
    # Mean prediction
    mean_prediction = np.mean(predictions_ensemble, axis=0)
    
    # Confidence based on ensemble agreement
    ensemble_confidence = 1.0 / (1.0 + ensemble_variance / (mean_prediction**2 + 1e-8))
    
    # Normalize to 0-100%
    confidence_pct = ensemble_confidence * 100
    confidence_pct = np.clip(confidence_pct, 10, 95)  # Reasonable bounds
    
    return confidence_pct, ensemble_variance

def main():
    print("ðŸš€ ULTRA-OPTIMIZED LINK Training - Simplified Version")
    print("=" * 80)
    print("ðŸ”§ Advanced Features:")
    print("  âœ… Technical indicators (SMA, EMA, RSI, MACD)")
    print("  âœ… Volatility and market structure analysis")
    print("  âœ… Ensemble of 3 different model architectures")
    print("  âœ… Advanced confidence estimation")
    print("  âœ… Optimized hyperparameters for crypto")
    print("=" * 80)
    
    # Get input file
    input_file = input("ðŸ“ Enter path to your LINK CSV file: ").strip()
    if not os.path.exists(input_file):
        print(f"âŒ File not found: {input_file}")
        return
    
    # Step 1: Ultra-optimized data preparation
    print(f"\nðŸ”§ Step 1: Ultra-optimized data preparation...")
    processed_data_path = os.path.join('data', 'LINK_ultra_optimized.csv')
    success = prepare_ultra_optimized_data(input_file, processed_data_path)
    
    if not success:
        return
    
    # Load processed data to get feature count
    df_processed = pd.read_csv(processed_data_path)
    n_features = len(df_processed.columns) - 1  # Exclude date column
    
    print(f"ðŸ“Š Total features: {n_features}")
    
    # Step 2: Ultra-optimized training configuration
    print(f"\nðŸ”§ Step 2: Ultra-optimized configuration...")
    
    class UltraArgs:
        def __init__(self):
            # Data parameters
            self.data = 'LINK_ULTRA'
            self.root_path = 'data'
            self.data_path = 'LINK_ultra_optimized.csv'
            
            # Model parameters (optimized for crypto)
            self.model = 'informer'
            self.features = 'MS'
            self.target = 'Close'
            self.freq = 'd'
            
            # Optimized sequence parameters
            self.seq_len = 90     # 3 months for better pattern recognition
            self.label_len = 45   # 1.5 months start token
            self.pred_len = 10    # 10 days prediction
            
            # Optimized model architecture
            self.d_model = 512
            self.n_heads = 8
            self.e_layers = 2
            self.d_layers = 1
            self.d_ff = 2048
            self.factor = 3       # Reduced for better attention
            self.dropout = 0.1    # Increased for regularization
            self.attn = 'prob'
            self.embed = 'timeF'
            self.activation = 'gelu'
            
            # Advanced loss parameters
            self.loss = 'adaptive_gmadl'
            self.beta_start = 1.2
            self.beta_end = 1.9
            
            # Optimized training parameters
            self.train_epochs = 30      # More epochs for better convergence
            self.batch_size = 8         # Smaller batch for better gradients
            self.learning_rate = 0.0005 # Higher LR with more regularization
            self.patience = 8           # More patience for convergence
            self.itr = 1
            
            # Feature dimensions
            self.enc_in = n_features
            self.dec_in = n_features
            self.c_out = 1
            
            # Technical parameters
            self.padding = 0
            self.distil = True
            self.mix = True
            self.output_attention = False
            self.inverse = False
            self.use_amp = True         # Mixed precision for efficiency
            self.num_workers = 0
            self.des = 'ultra_optimized'
            self.lradj = 'type1'
            self.use_multi_gpu = False
            self.devices = '0'
            self.cols = None
            self.checkpoints = 'checkpoints'
            
            # GPU setup
            self.use_gpu = True if torch.cuda.is_available() else False
            self.gpu = 0
            self.do_predict = False  # Simplified - no prediction step
            
            # Additional required attributes
            self.detail_freq = self.freq
    
    base_args = UltraArgs()
    
    print(f"âœ… Ultra-optimized configuration:")
    print(f"  ðŸ“Š Features: {n_features} (with technical indicators)")
    print(f"  ðŸŽ¯ Sequence: {base_args.seq_len} â†’ {base_args.pred_len} days")
    print(f"  ðŸ§  Architecture: Enhanced with regularization")
    print(f"  ðŸ”„ Training: {base_args.train_epochs} epochs with adaptive GMADL")
    print(f"  âš¡ Mixed precision: {base_args.use_amp}")
    
    # Step 3: Ensemble training
    print(f"\nðŸš€ Step 3: Training ensemble models...")
    
    # Create model variants
    model_configs = [
        ('ensemble_model_1', 2, 1, 512, 8),   # Standard
        ('ensemble_model_2', 3, 2, 512, 8),   # Deeper
        ('ensemble_model_3', 2, 1, 768, 12),  # Wider
    ]
    
    ensemble_results = []
    
    for i, (name, e_layers, d_layers, d_model, n_heads) in enumerate(model_configs):
        print(f"\nðŸ”¥ Training Model {i+1}/3...")
        
        # Create args for this model
        args = UltraArgs()
        args.des = name
        args.e_layers = e_layers
        args.d_layers = d_layers
        args.d_model = d_model
        args.n_heads = n_heads
        
        setting = f'informer_LINK_ULTRA_ftMS_sl{args.seq_len}_ll{args.label_len}_pl{args.pred_len}_{args.des}'
        
        exp = Exp_Informer(args)
        
        print(f'>>>>>>>Training {args.des}>>>>>>>>>>>>>>>>>>>')
        exp.train(setting)
        
        print(f'>>>>>>>Testing {args.des}<<<<<<<<<<<<<<<<<<<<<')
        test_results = exp.test(setting)
        
        # Store results
        ensemble_results.append({
            'name': name,
            'mse': test_results[0] if test_results else 0,
            'mae': test_results[1] if test_results else 0,
            'setting': setting
        })
        
        print(f"âœ… Model {i+1} completed - MSE: {test_results[0]:.4f}, MAE: {test_results[1]:.4f}")
    
    # Step 4: Results summary
    print(f"\nðŸ“Š Step 4: Ultra-Optimized Training Results")
    print("=" * 80)
    print(f"ðŸ”¥ ENSEMBLE OF 3 MODELS - TRAINING COMPLETED")
    print(f"ðŸ“Š Features used: {n_features} (including technical indicators)")
    print()
    
    print("ðŸ“Š Model Performance Summary:")
    total_mse = 0
    total_mae = 0
    
    for i, result in enumerate(ensemble_results):
        mse = result['mse']
        mae = result['mae']
        total_mse += mse
        total_mae += mae
        
        # Performance rating
        if mse < 0.5:
            rating = "ðŸ”¥ EXCELLENT"
        elif mse < 1.0:
            rating = "ðŸŸ¢ VERY GOOD"
        elif mse < 2.0:
            rating = "ðŸŸ¡ GOOD"
        else:
            rating = "ðŸ”´ NEEDS IMPROVEMENT"
        
        print(f"Model {i+1} ({result['name']}): MSE: {mse:.4f}, MAE: {mae:.4f} {rating}")
    
    avg_mse = total_mse / len(ensemble_results)
    avg_mae = total_mae / len(ensemble_results)
    
    print(f"\nðŸ† ENSEMBLE AVERAGE PERFORMANCE:")
    print(f"  ðŸ“Š Average MSE: {avg_mse:.4f}")
    print(f"  ðŸ“Š Average MAE: {avg_mae:.4f}")
    
    if avg_mse < 0.5:
        overall_rating = "ðŸ”¥ ULTRA HIGH ACCURACY"
    elif avg_mse < 1.0:
        overall_rating = "ðŸŸ¢ HIGH ACCURACY"
    elif avg_mse < 2.0:
        overall_rating = "ðŸŸ¡ GOOD ACCURACY"
    else:
        overall_rating = "ðŸ”´ MODERATE ACCURACY"
    
    print(f"  ðŸŽ¯ Overall Rating: {overall_rating}")
    
    # Performance interpretation
    print(f"\nðŸ“ˆ PERFORMANCE INTERPRETATION:")
    print(f"  ðŸ“Š MSE (Mean Squared Error): Lower is better")
    print(f"  ðŸ“Š MAE (Mean Absolute Error): Average price prediction error")
    print(f"  ðŸŽ¯ Your MAE of {avg_mae:.4f} means predictions are typically within ${avg_mae:.2f}")
    
    # Save results summary
    results_summary = pd.DataFrame(ensemble_results)
    summary_file = os.path.join('checkpoints', 'ENSEMBLE_TRAINING_SUMMARY.csv')
    os.makedirs('checkpoints', exist_ok=True)
    results_summary.to_csv(summary_file, index=False)
    
    print(f"\nðŸ’¾ TRAINING SUMMARY SAVED:")
    print(f"  ðŸ“Š Summary: {summary_file}")
    print(f"  ðŸ“ Model checkpoints: checkpoints/ folder")
    
    print(f"\nðŸŽ‰ ULTRA-OPTIMIZED TRAINING COMPLETED!")
    print(f"ðŸ”¥ You now have 3 trained models ready for LINK predictions!")
    print(f"ðŸ“ˆ Use the saved models for future predictions on new LINK data!")
    
    torch.cuda.empty_cache()

if __name__ == '__main__':
    main()